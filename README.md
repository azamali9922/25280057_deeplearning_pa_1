<p align="center">
  <img src="images/lums%20logo.png" alt="LUMS Logo" width="150"/>
</p>

<h1 align="center">ğŸ§  AI600 â€” Deep Learning: Assignment 1</h1>

<p align="center">
  <b>Feedforward Neural Networks from Scratch using NumPy</b><br/>
  Lahore University of Management Sciences Â· Spring 2026
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.x-blue?logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/NumPy-MLP-orange?logo=numpy" alt="NumPy"/>
  <img src="https://img.shields.io/badge/LaTeX-Report-green?logo=latex" alt="LaTeX"/>
  <img src="https://img.shields.io/badge/Status-Submitted-brightgreen" alt="Status"/>
</p>

---

## ğŸ“‹ Overview

A **multi-class feedforward neural network** built entirely from scratch using **NumPy** â€” no PyTorch, no TensorFlow. The task is to classify Airbnb listings into 4 price categories using tabular features like neighbourhood, room type, amenity score, and more.

### Key Results

| Metric | ReLU | Sigmoid |
|--------|------|---------|
| Train Accuracy | **81.55%** | 56.32% |
| Val Accuracy | **80.92%** | 56.32% |
| Test Accuracy | **38.82%** | â€” |

> The large trainâ†’test gap (42.7%) reveals a distribution shift caused by missing-value imputation patterns in training data that don't exist in the clean test set.

---

## ğŸ—ï¸ Architecture

```
Input (13) â†’ Dense(64, ReLU) â†’ Dense(32, ReLU) â†’ Dense(4, Softmax)
```

- **Loss:** Cross-entropy with softmax
- **Optimizer:** Batch gradient descent (lr=0.01, 200 iterations)
- **Preprocessing:** One-hot encoding, z-score normalization, median/mode imputation

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ part1.ipynb              # All code â€” EDA, MLP, training, evaluation
â”œâ”€â”€ 25280057_report.tex      # LaTeX source for the report
â”œâ”€â”€ 25280057_report.pdf      # Compiled PDF report
â”œâ”€â”€ train.csv                # Training dataset (41,348 samples)
â”œâ”€â”€ test.csv                 # Test dataset (7,297 samples)
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ images/                  # Plots and figures used in the report
â”‚   â”œâ”€â”€ training_validation_acc.png
â”‚   â”œâ”€â”€ reluvssigmoidgradient.png
â”‚   â”œâ”€â”€ correlationmatrix.png
â”‚   â”œâ”€â”€ testsetconfusion.png
â”‚   â”œâ”€â”€ gradientbasedfeatures.png
â”‚   â”œâ”€â”€ featureattribution_trainvstest.png
â”‚   â””â”€â”€ ...
â””â”€â”€ README.md
```

---

## ğŸ”¬ Assignment Parts

### Part A â€” Exploratory Data Analysis
- Dataset inspection, missing value analysis (5,406 NaN values across 6 columns)
- Class imbalance: Class 1 dominates at 56.3%, Class 3 only 6.4%
- Feature distributions via box plots, count plots, and correlation matrix
- One-hot encoding expands features from 6 â†’ 13

### Part B(a) â€” MLP Implementation & Training
- Forward pass: matrix multiplications + activation functions
- Backward pass: manual chain-rule gradient computation
- Compared **ReLU** vs **Sigmoid** activations
- ReLU reaches 81.5% accuracy; Sigmoid plateaus at 56.3% (majority-class baseline)

### Part B(b) â€” Gradient Magnitude Analysis
- Tracked `|âˆ‡Wâ‚|` and `|âˆ‡Wâ‚‚|` across all 200 iterations
- Sigmoid shows classic **vanishing gradient** â€” Layer 1 gradients are 2â€“6Ã— smaller than Layer 2
- ReLU maintains healthy gradient flow with ratio close to 0.5

### Part C(a) â€” Gradient-Based Feature Attribution
- Implemented Aggarwal Â§2.8 method: `Avg |âˆ‚oâ‚˜/âˆ‚xáµ¢|` over correctly classified samples
- Top feature: `amenity_score` (0.354 for ReLU, 0.0096 for Sigmoid)
- Sigmoid attributions are compressed due to vanishing gradients

### Part C(b) â€” Handwritten Backpropagation
- Full chain-rule derivation for the 2-hidden-layer network
- Pseudocode for the backward pass

### Part D â€” Test Evaluation & Generalization
- Test accuracy drops to **38.8%** despite 81.5% training accuracy
- Classification report shows Class 3 (minority) F1-score of just 0.09
- Feature attribution shift analysis: all features show increased importance on test data
- Root cause: imputation artifacts in training data create patterns absent in test data

### Question 2 â€” Shared Bias Gradient Derivation
- Mathematical proof that shared bias gradient = sum of individual neuron gradients
- Analysis of convergence implications

---

## ğŸš€ Quick Start

```bash
# Clone the repo
git clone https://github.com/azamali9922/25280057_deeplearning_pa_1.git
cd 25280057_deeplearning_pa_1

# Create virtual environment
python -m venv ai600_env
ai600_env\Scripts\activate    # Windows
# source ai600_env/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Open the notebook
jupyter notebook part1.ipynb
```

---

## ğŸ“Š Selected Visualizations

<p align="center">
  <img src="images/training_validation_acc.png" width="45%" alt="Training Curves"/>
  <img src="images/reluvssigmoidgradient.png" width="45%" alt="Gradient Magnitudes"/>
</p>

<p align="center">
  <img src="images/testsetconfusion.png" width="35%" alt="Confusion Matrix"/>
  <img src="images/featureattribution_trainvstest.png" width="55%" alt="Feature Attribution"/>
</p>

---

## ğŸ‘¤ Author

| | |
|---|---|
| **Name** | Azam Ali |
| **Roll No.** | 25280057 |
| **Course** | AI600 â€” Deep Learning |
| **Institution** | LUMS, School of Science & Engineering |
| **Semester** | Spring 2026 |

---

