\documentclass[11pt,a4paper]{article}

% ── Packages ──
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{booktabs}

% ── Macros ──
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bdelta}{\boldsymbol{\delta}}
\newcommand{\odot}{\circ}

\title{\textbf{Part C (a): Gradient-Based Feature Attribution}\\[4pt]
\large Analytical Derivation of $\pd{L}{\bx}$ via Modified Backpropagation}
\author{25280057}
\date{}

\begin{document}
\maketitle

% ================================================================
\section{Network Architecture Recap}
% ================================================================

We consider a fully-connected feedforward network with two hidden layers and a softmax output, identical to the one studied in class:

\begin{align}
    \ba^{(1)} &= \bW^{(1)}\bx^{(0)} + \bb^{(1)},
        & \bx^{(1)} &= g\!\left(\ba^{(1)}\right) \label{eq:layer1}\\[4pt]
    \ba^{(2)} &= \bW^{(2)}\bx^{(1)} + \bb^{(2)},
        & \bx^{(2)} &= g\!\left(\ba^{(2)}\right) \label{eq:layer2}\\[4pt]
    \ba^{(3)} &= \bW^{(3)}\bx^{(2)} + \bb^{(3)},
        & \hat{\mathbf{y}} = \bx^{(3)} &= \mathrm{softmax}\!\left(\ba^{(3)}\right) \label{eq:layer3}
\end{align}

where $\bx^{(0)} \in \mathbb{R}^{M_0}$ is the input, $g(\cdot)$ is the hidden-layer activation (ReLU or sigmoid), and the loss is cross-entropy:
\begin{equation}\label{eq:loss}
    L = -\sum_{k=1}^{K} y_k \log \hat{y}_k.
\end{equation}

% ================================================================
\section{Standard Backpropagation (Review)}
% ================================================================

Define $\bdelta^{(\ell)} \triangleq \pd{L}{\ba^{(\ell)}}$ (gradient of the loss w.r.t.\ the pre-activation at layer $\ell$).  From class we have:

\begin{align}
    \bdelta^{(3)} &= \hat{\mathbf{y}} - \mathbf{y}
        \label{eq:delta3}\\[4pt]
    \bdelta^{(2)} &= \left({\bW^{(3)}}^\top \bdelta^{(3)}\right) \odot\, g'\!\left(\ba^{(2)}\right)
        \label{eq:delta2}\\[4pt]
    \bdelta^{(1)} &= \left({\bW^{(2)}}^\top \bdelta^{(2)}\right) \odot\, g'\!\left(\ba^{(1)}\right)
        \label{eq:delta1}
\end{align}

Standard backprop stops here because $\bW^{(\ell)}$ and $\bb^{(\ell)}$ are the \emph{trainable} parameters.  The input $\bx^{(0)}$ is normally treated as fixed data.  To obtain feature attributions we simply \textbf{continue the chain one more step} back to $\bx^{(0)}$.

% ================================================================
\section{Deriving $\pd{L}{\bx^{(0)}}$ — Step by Step}
% ================================================================

\subsection{Step 1: Relate $\ba^{(1)}$ to $\bx^{(0)}$}

From Eq.~\eqref{eq:layer1}:
\begin{equation}
    \ba^{(1)} = \bW^{(1)}\bx^{(0)} + \bb^{(1)}
    \quad\Longrightarrow\quad
    \pd{\ba^{(1)}}{\bx^{(0)}} = \bW^{(1)}.
\end{equation}

\subsection{Step 2: Apply the chain rule}

Using the chain rule through the first layer:
\begin{equation}
    \pd{L}{\bx^{(0)}}
    = \pd{\ba^{(1)}}{\bx^{(0)}}^{\!\top} \;\pd{L}{\ba^{(1)}}
    = {\bW^{(1)}}^\top \bdelta^{(1)}.
    \label{eq:dLdx}
\end{equation}

\subsection{Step 3: Expand $\bdelta^{(1)}$ fully}

Substituting Eqs.~\eqref{eq:delta3}--\eqref{eq:delta1} into \eqref{eq:dLdx}:

\begin{equation}
\boxed{
    \pd{L}{\bx^{(0)}}
    = {\bW^{(1)}}^\top
      \Bigl[
        \Bigl(
          {\bW^{(2)}}^\top
          \bigl(
            {\bW^{(3)}}^\top
            (\hat{\mathbf{y}} - \mathbf{y})
          \bigr)
          \odot\, g'\!\bigl(\ba^{(2)}\bigr)
        \Bigr)
        \odot\, g'\!\bigl(\ba^{(1)}\bigr)
      \Bigr]
}
\label{eq:full}
\end{equation}

This is a vector in $\mathbb{R}^{M_0}$.  Its $i$-th component is $\pd{L}{x_i^{(0)}}$, the sensitivity of the loss to the $i$-th input feature.

\subsection{Component-wise form}

For a single input feature $x_i^{(0)}$:

\begin{equation}
    \pd{L}{x_i^{(0)}}
    = \sum_{j=1}^{M_1} W^{(1)}_{ji}\;\delta^{(1)}_j
    = \mathbf{e}_i^\top\, {\bW^{(1)}}^\top \bdelta^{(1)}
\end{equation}

where $\mathbf{e}_i$ is the $i$-th standard basis vector and $\delta^{(1)}_j$ is the $j$-th element of $\bdelta^{(1)}$.

% ================================================================
\section{Modified Backpropagation Procedure}
% ================================================================

The modification to the standard algorithm is minimal — we add \textbf{one extra step} at the end:

\begin{enumerate}[leftmargin=2em]
    \item \textbf{Forward pass} — compute $\ba^{(\ell)}$, $\bx^{(\ell)}$ for $\ell = 1,2,3$ (unchanged).
    \item \textbf{Backward pass (layers 3 $\to$ 1)} — compute $\bdelta^{(3)}, \bdelta^{(2)}, \bdelta^{(1)}$ (unchanged).
    \item \textbf{Extra step (layer 1 $\to$ input)} — compute:
    \[
        \pd{L}{\bx^{(0)}} = {\bW^{(1)}}^\top \bdelta^{(1)}.
    \]
\end{enumerate}

No additional activation derivative is needed at the input layer because $\bx^{(0)}$ enters the network \emph{linearly} (there is no activation applied to the raw input).

% ================================================================
\section{Pseudocode}
% ================================================================

\begin{algorithm}[H]
\caption{Gradient-Based Feature Attribution via Modified Backpropagation}
\label{alg:attribution}
\begin{algorithmic}[1]
\Require Training set $\{(\bx^{(0)}_n, \mathbf{y}_n)\}_{n=1}^{N}$, trained parameters $\{\bW^{(\ell)}, \bb^{(\ell)}\}_{\ell=1}^{3}$, activation $g$
\Ensure Feature importance scores $\mathbf{s} \in \mathbb{R}^{M_0}$

\State $\mathbf{S} \gets \mathbf{0}_{M_0}$ \Comment{Accumulator for gradient magnitudes}

\For{$n = 1$ \textbf{to} $N$}
    \Statex \hspace{1.5em}\texttt{// --- Forward Pass ---}
    \State $\ba^{(1)} \gets \bW^{(1)}\bx^{(0)}_n + \bb^{(1)};\quad \bx^{(1)} \gets g(\ba^{(1)})$
    \State $\ba^{(2)} \gets \bW^{(2)}\bx^{(1)} + \bb^{(2)};\quad \bx^{(2)} \gets g(\ba^{(2)})$
    \State $\ba^{(3)} \gets \bW^{(3)}\bx^{(2)} + \bb^{(3)};\quad \hat{\mathbf{y}} \gets \text{softmax}(\ba^{(3)})$

    \Statex \hspace{1.5em}\texttt{// --- Standard Backward Pass ---}
    \State $\bdelta^{(3)} \gets \hat{\mathbf{y}} - \mathbf{y}_n$
    \State $\bdelta^{(2)} \gets ({\bW^{(3)}}^\top \bdelta^{(3)}) \odot g'(\ba^{(2)})$
    \State $\bdelta^{(1)} \gets ({\bW^{(2)}}^\top \bdelta^{(2)}) \odot g'(\ba^{(1)})$

    \Statex \hspace{1.5em}\texttt{// --- Extra Step: Gradient w.r.t.\ Input ---}
    \State $\mathbf{g}_n \gets {\bW^{(1)}}^\top \bdelta^{(1)}$
        \Comment{$\mathbf{g}_n = \pd{L}{\bx^{(0)}_n} \in \mathbb{R}^{M_0}$}

    \State $\mathbf{S} \gets \mathbf{S} + |\mathbf{g}_n|$
        \Comment{Element-wise absolute value}
\EndFor

\Statex
\State $\mathbf{s} \gets \dfrac{1}{N}\,\mathbf{S}$
    \Comment{Average gradient magnitude per feature}

\Statex
\Statex \texttt{// --- Rank Features ---}
\State $\text{ranking} \gets \text{argsort}(\mathbf{s},\;\text{descending})$
\State \Return $\mathbf{s}$, ranking
\end{algorithmic}
\end{algorithm}

The feature with the largest $s_i$ is deemed \emph{most important}; the one with the smallest $s_i$ is least important.

% ================================================================
\section{Why $\left|\pd{L}{x_i}\right|$ Is a Meaningful Measure}
% ================================================================

From an \textbf{optimization / first-order Taylor expansion} perspective:

\begin{equation}
    L(\bx^{(0)} + \boldsymbol{\epsilon})
    \approx L(\bx^{(0)})
    + \left\langle \pd{L}{\bx^{(0)}},\; \boldsymbol{\epsilon} \right\rangle.
\end{equation}

Therefore, if we perturb only the $i$-th feature by a small amount $\epsilon_i$:

\begin{equation}
    \Delta L \approx \pd{L}{x_i^{(0)}}\;\epsilon_i.
\end{equation}

This yields three concrete justifications:

\begin{enumerate}[leftmargin=2em]
    \item \textbf{Local sensitivity.}
    $\left|\pd{L}{x_i^{(0)}}\right|$ measures how much the loss changes per unit perturbation in $x_i$.  A large magnitude means the network's prediction is \emph{highly sensitive} to that feature — hence the feature contributes significantly to the output.

    \item \textbf{Gradient descent analogy.}
    In standard training, parameters with large gradient magnitudes receive the largest updates because they have the greatest impact on reducing the loss.  By the same logic, input features with large $\left|\pd{L}{x_i}\right|$ are the dimensions along which the loss landscape is steepest — they have the most influence on the model's decision boundary.

    \item \textbf{Connection to saliency maps.}
    This gradient-based attribution is formally equivalent to the \emph{saliency map} approach of Simonyan et al.\ (2014).  Averaging $|\pd{L}{x_i}|$ over the dataset aggregates per-sample sensitivities into a global feature importance ranking, smoothing out noise from individual examples.
\end{enumerate}

\medskip
\noindent
In summary, the magnitude $\left|\pd{L}{x_i}\right|$ is a principled, first-order measure of each input feature's contribution to the network's loss — and by extension, to its predictions.

\end{document}
